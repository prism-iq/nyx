"""
CIPHER LLM Integration
Large Language Model integration for sophisticated knowledge synthesis.

Provides:
1. Claim extraction from scientific text (more nuanced than regex/NLP)
2. Hypothesis generation from detected patterns
3. Cross-domain analogy detection
4. Natural language synthesis reports
5. Entity resolution and disambiguation

Supports multiple backends:
- Anthropic Claude (preferred for scientific reasoning)
- OpenAI GPT-4
- Local models via Ollama

Cross-domain bridge: All domains (LLM synthesis is inherently cross-domain)
"""

import asyncio
import json
import logging
import os
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Optional, List, Dict, Any, Tuple

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """Supported LLM providers"""
    ANTHROPIC = "anthropic"
    OPENAI = "openai"
    OLLAMA = "ollama"


class LLMModel(Enum):
    """Available models by provider"""
    # Anthropic
    CLAUDE_SONNET = "claude-sonnet-4-20250514"
    CLAUDE_HAIKU = "claude-3-5-haiku-20241022"
    # OpenAI
    GPT4O = "gpt-4o"
    GPT4O_MINI = "gpt-4o-mini"
    # Ollama (local)
    LLAMA3 = "llama3"
    MISTRAL = "mistral"


@dataclass
class LLMConfig:
    """LLM configuration"""
    provider: LLMProvider = LLMProvider.ANTHROPIC
    model: str = LLMModel.CLAUDE_SONNET.value
    api_key: Optional[str] = None
    base_url: Optional[str] = None  # For Ollama
    max_tokens: int = 4096
    temperature: float = 0.3  # Lower for more deterministic scientific output

    @classmethod
    def from_env(cls) -> 'LLMConfig':
        """Create config from environment variables."""
        provider_str = os.getenv("CIPHER_LLM_PROVIDER", "anthropic").lower()
        provider = LLMProvider(provider_str)

        if provider == LLMProvider.ANTHROPIC:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            model = os.getenv("CIPHER_LLM_MODEL", LLMModel.CLAUDE_SONNET.value)
        elif provider == LLMProvider.OPENAI:
            api_key = os.getenv("OPENAI_API_KEY")
            model = os.getenv("CIPHER_LLM_MODEL", LLMModel.GPT4O.value)
        else:  # Ollama
            api_key = None
            model = os.getenv("CIPHER_LLM_MODEL", LLMModel.LLAMA3.value)

        return cls(
            provider=provider,
            model=model,
            api_key=api_key,
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            max_tokens=int(os.getenv("CIPHER_LLM_MAX_TOKENS", "4096")),
            temperature=float(os.getenv("CIPHER_LLM_TEMPERATURE", "0.3"))
        )


@dataclass
class ExtractedClaimLLM:
    """A claim extracted by LLM"""
    claim_text: str
    claim_type: str  # hypothesis, finding, method, definition, observation
    confidence: float
    evidence_strength: str  # weak, moderate, strong, definitive
    entities: List[str]
    causal_relations: List[Dict[str, str]]
    hedging_level: float  # 0 = certain, 1 = highly hedged
    domains: List[str]
    methodology: Optional[str] = None
    statistical_info: Optional[Dict[str, Any]] = None


@dataclass
class GeneratedHypothesis:
    """A hypothesis generated by LLM"""
    hypothesis_text: str
    source_patterns: List[str]
    domains_involved: List[str]
    testability: float  # 0-1 how testable
    novelty: float  # 0-1 how novel
    supporting_reasoning: str
    suggested_experiments: List[str]
    potential_implications: List[str]
    confidence: float


@dataclass
class CrossDomainAnalogy:
    """A cross-domain analogy detected by LLM"""
    source_domain: str
    target_domain: str
    source_concept: str
    target_concept: str
    analogy_description: str
    mapping_details: Dict[str, str]
    strength: float  # 0-1
    limitations: List[str]
    research_opportunities: List[str]


@dataclass
class SynthesisReport:
    """A synthesis report generated by LLM"""
    title: str
    executive_summary: str
    key_findings: List[str]
    cross_domain_insights: List[str]
    contradictions_noted: List[str]
    knowledge_gaps: List[str]
    future_directions: List[str]
    full_report: str
    generated_at: datetime = field(default_factory=datetime.now)


class LLMBackend(ABC):
    """Abstract base class for LLM backends"""

    @abstractmethod
    async def generate(self, prompt: str, system: str = None) -> str:
        """Generate text from prompt."""
        pass

    @abstractmethod
    async def generate_json(self, prompt: str, system: str = None) -> Dict:
        """Generate JSON response from prompt."""
        pass


class AnthropicBackend(LLMBackend):
    """Anthropic Claude backend"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self._client = None

    def _get_client(self):
        if self._client is None:
            try:
                import anthropic
                self._client = anthropic.Anthropic(api_key=self.config.api_key)
            except ImportError:
                raise ImportError("anthropic package required. Install with: pip install anthropic")
        return self._client

    async def generate(self, prompt: str, system: str = None) -> str:
        client = self._get_client()

        messages = [{"role": "user", "content": prompt}]

        # Run sync client in executor
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                system=system or "You are a scientific research assistant specializing in cross-domain knowledge synthesis.",
                messages=messages
            )
        )

        return response.content[0].text

    async def generate_json(self, prompt: str, system: str = None) -> Dict:
        json_prompt = prompt + "\n\nRespond with valid JSON only. No other text."
        response = await self.generate(json_prompt, system)

        # Extract JSON from response
        try:
            # Try to parse directly
            return json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from markdown code block
            match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response)
            if match:
                return json.loads(match.group(1))
            # Try to find JSON object
            match = re.search(r'\{[\s\S]*\}', response)
            if match:
                return json.loads(match.group(0))
            raise ValueError(f"Could not parse JSON from response: {response[:200]}")


class OpenAIBackend(LLMBackend):
    """OpenAI GPT backend"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self._client = None

    def _get_client(self):
        if self._client is None:
            try:
                import openai
                self._client = openai.OpenAI(api_key=self.config.api_key)
            except ImportError:
                raise ImportError("openai package required. Install with: pip install openai")
        return self._client

    async def generate(self, prompt: str, system: str = None) -> str:
        client = self._get_client()

        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: client.chat.completions.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                messages=messages
            )
        )

        return response.choices[0].message.content

    async def generate_json(self, prompt: str, system: str = None) -> Dict:
        json_prompt = prompt + "\n\nRespond with valid JSON only."
        response = await self.generate(json_prompt, system)

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response)
            if match:
                return json.loads(match.group(1))
            match = re.search(r'\{[\s\S]*\}', response)
            if match:
                return json.loads(match.group(0))
            raise ValueError(f"Could not parse JSON from response: {response[:200]}")


class OllamaBackend(LLMBackend):
    """Ollama local model backend"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.base_url = config.base_url or "http://localhost:11434"

    async def generate(self, prompt: str, system: str = None) -> str:
        import aiohttp

        url = f"{self.base_url}/api/generate"
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "system": system or "You are a scientific research assistant.",
            "stream": False,
            "options": {
                "temperature": self.config.temperature,
                "num_predict": self.config.max_tokens
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as resp:
                if resp.status != 200:
                    raise RuntimeError(f"Ollama error: {await resp.text()}")
                data = await resp.json()
                return data["response"]

    async def generate_json(self, prompt: str, system: str = None) -> Dict:
        json_prompt = prompt + "\n\nRespond with valid JSON only."
        response = await self.generate(json_prompt, system)

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            match = re.search(r'\{[\s\S]*\}', response)
            if match:
                return json.loads(match.group(0))
            raise ValueError(f"Could not parse JSON from response: {response[:200]}")


class LLMIntegration:
    """
    Main LLM integration class for CIPHER.

    Provides high-level methods for:
    - Claim extraction from scientific text
    - Hypothesis generation from patterns
    - Cross-domain analogy detection
    - Synthesis report generation
    """

    # System prompts for different tasks
    SYSTEM_CLAIM_EXTRACTION = """You are a scientific claim extraction expert.
Your task is to identify and extract specific claims from scientific text.
Focus on: hypotheses, findings, methods, definitions, and observations.
Be precise about confidence levels and evidence strength.
Identify hedging language and causal relationships.
Output structured JSON."""

    SYSTEM_HYPOTHESIS_GENERATION = """You are a scientific hypothesis generator.
Given patterns observed across domains, generate novel, testable hypotheses.
Consider cross-domain analogies and unexpected connections.
Be creative but grounded in the evidence provided.
Output structured JSON."""

    SYSTEM_ANALOGY_DETECTION = """You are an expert at finding cross-domain analogies.
Identify structural, functional, or conceptual similarities between different domains.
Be specific about the mapping between concepts.
Note both strengths and limitations of analogies.
Output structured JSON."""

    SYSTEM_SYNTHESIS = """You are a scientific synthesis expert.
Create comprehensive reports that integrate knowledge across domains.
Highlight key findings, contradictions, and gaps.
Provide actionable future directions.
Write in clear, academic prose."""

    def __init__(self, config: LLMConfig = None):
        """
        Initialize LLM integration.

        Args:
            config: LLM configuration. If None, loads from environment.
        """
        self.config = config or LLMConfig.from_env()
        self._backend: Optional[LLMBackend] = None

    @property
    def backend(self) -> LLMBackend:
        """Get or create the LLM backend."""
        if self._backend is None:
            if self.config.provider == LLMProvider.ANTHROPIC:
                self._backend = AnthropicBackend(self.config)
            elif self.config.provider == LLMProvider.OPENAI:
                self._backend = OpenAIBackend(self.config)
            elif self.config.provider == LLMProvider.OLLAMA:
                self._backend = OllamaBackend(self.config)
            else:
                raise ValueError(f"Unknown provider: {self.config.provider}")
        return self._backend

    async def extract_claims(
        self,
        text: str,
        title: str = "",
        domains: List[str] = None
    ) -> List[ExtractedClaimLLM]:
        """
        Extract claims from scientific text using LLM.

        Args:
            text: The scientific text (abstract, paper content)
            title: Paper title for context
            domains: Expected domains for the content

        Returns:
            List of extracted claims
        """
        domains_str = ", ".join(domains) if domains else "unknown"

        prompt = f"""Extract scientific claims from the following text.

Title: {title}
Expected domains: {domains_str}

Text:
{text[:4000]}

For each claim, provide:
1. claim_text: The exact claim
2. claim_type: One of [hypothesis, finding, method, definition, observation]
3. confidence: 0-1 how confident the claim is stated
4. evidence_strength: One of [weak, moderate, strong, definitive]
5. entities: List of key scientific entities mentioned
6. causal_relations: List of {{cause, effect, relation_type}} if any
7. hedging_level: 0-1 where 0=certain, 1=highly hedged
8. domains: Which domains this claim relates to
9. methodology: How was this established (if mentioned)
10. statistical_info: Any p-values, sample sizes, effect sizes

Return as JSON array: {{"claims": [...]}}"""

        try:
            result = await self.backend.generate_json(prompt, self.SYSTEM_CLAIM_EXTRACTION)

            claims = []
            for c in result.get("claims", []):
                claims.append(ExtractedClaimLLM(
                    claim_text=c.get("claim_text", ""),
                    claim_type=c.get("claim_type", "observation"),
                    confidence=float(c.get("confidence", 0.5)),
                    evidence_strength=c.get("evidence_strength", "moderate"),
                    entities=c.get("entities", []),
                    causal_relations=c.get("causal_relations", []),
                    hedging_level=float(c.get("hedging_level", 0.5)),
                    domains=c.get("domains", domains or []),
                    methodology=c.get("methodology"),
                    statistical_info=c.get("statistical_info")
                ))

            return claims

        except Exception as e:
            logger.error(f"LLM claim extraction failed: {e}")
            return []

    async def generate_hypotheses(
        self,
        patterns: List[Dict[str, Any]],
        claims: List[Dict[str, Any]],
        num_hypotheses: int = 5
    ) -> List[GeneratedHypothesis]:
        """
        Generate novel hypotheses from observed patterns.

        Args:
            patterns: Detected patterns from the knowledge base
            claims: Related claims for context
            num_hypotheses: Number of hypotheses to generate

        Returns:
            List of generated hypotheses
        """
        patterns_str = json.dumps(patterns[:10], indent=2)
        claims_str = json.dumps(claims[:20], indent=2)

        prompt = f"""Based on the following patterns and claims from a cross-domain knowledge base,
generate {num_hypotheses} novel, testable scientific hypotheses.

Detected Patterns:
{patterns_str}

Related Claims:
{claims_str}

For each hypothesis, provide:
1. hypothesis_text: The hypothesis statement
2. source_patterns: Which patterns inspired this
3. domains_involved: Which scientific domains are involved
4. testability: 0-1 how testable is this hypothesis
5. novelty: 0-1 how novel (not obvious from existing knowledge)
6. supporting_reasoning: Why this hypothesis is plausible
7. suggested_experiments: How to test this
8. potential_implications: What would this mean if true
9. confidence: 0-1 confidence in this hypothesis

Focus on:
- Cross-domain connections that aren't obvious
- Testable predictions
- High scientific value

Return as JSON: {{"hypotheses": [...]}}"""

        try:
            result = await self.backend.generate_json(prompt, self.SYSTEM_HYPOTHESIS_GENERATION)

            hypotheses = []
            for h in result.get("hypotheses", []):
                hypotheses.append(GeneratedHypothesis(
                    hypothesis_text=h.get("hypothesis_text", ""),
                    source_patterns=h.get("source_patterns", []),
                    domains_involved=h.get("domains_involved", []),
                    testability=float(h.get("testability", 0.5)),
                    novelty=float(h.get("novelty", 0.5)),
                    supporting_reasoning=h.get("supporting_reasoning", ""),
                    suggested_experiments=h.get("suggested_experiments", []),
                    potential_implications=h.get("potential_implications", []),
                    confidence=float(h.get("confidence", 0.5))
                ))

            return hypotheses

        except Exception as e:
            logger.error(f"LLM hypothesis generation failed: {e}")
            return []

    async def detect_analogies(
        self,
        domain_a: str,
        domain_b: str,
        claims_a: List[Dict[str, Any]],
        claims_b: List[Dict[str, Any]],
        num_analogies: int = 5
    ) -> List[CrossDomainAnalogy]:
        """
        Detect cross-domain analogies between two domains.

        Args:
            domain_a: First domain name
            domain_b: Second domain name
            claims_a: Claims from first domain
            claims_b: Claims from second domain
            num_analogies: Number of analogies to find

        Returns:
            List of detected analogies
        """
        claims_a_str = json.dumps(claims_a[:15], indent=2)
        claims_b_str = json.dumps(claims_b[:15], indent=2)

        prompt = f"""Find {num_analogies} cross-domain analogies between {domain_a} and {domain_b}.

Claims from {domain_a}:
{claims_a_str}

Claims from {domain_b}:
{claims_b_str}

For each analogy, provide:
1. source_domain: The source domain
2. target_domain: The target domain
3. source_concept: The concept in source domain
4. target_concept: The analogous concept in target domain
5. analogy_description: Description of the analogy
6. mapping_details: Dict mapping specific elements
7. strength: 0-1 how strong is the analogy
8. limitations: What are the limits of this analogy
9. research_opportunities: What research could this analogy inspire

Focus on:
- Structural analogies (similar patterns/processes)
- Functional analogies (similar purposes/outcomes)
- Unexpected but valid connections

Return as JSON: {{"analogies": [...]}}"""

        try:
            result = await self.backend.generate_json(prompt, self.SYSTEM_ANALOGY_DETECTION)

            analogies = []
            for a in result.get("analogies", []):
                analogies.append(CrossDomainAnalogy(
                    source_domain=a.get("source_domain", domain_a),
                    target_domain=a.get("target_domain", domain_b),
                    source_concept=a.get("source_concept", ""),
                    target_concept=a.get("target_concept", ""),
                    analogy_description=a.get("analogy_description", ""),
                    mapping_details=a.get("mapping_details", {}),
                    strength=float(a.get("strength", 0.5)),
                    limitations=a.get("limitations", []),
                    research_opportunities=a.get("research_opportunities", [])
                ))

            return analogies

        except Exception as e:
            logger.error(f"LLM analogy detection failed: {e}")
            return []

    async def generate_synthesis_report(
        self,
        topic: str,
        claims: List[Dict[str, Any]],
        patterns: List[Dict[str, Any]],
        contradictions: List[Dict[str, Any]] = None,
        gaps: List[Dict[str, Any]] = None
    ) -> SynthesisReport:
        """
        Generate a comprehensive synthesis report.

        Args:
            topic: The topic/focus of the report
            claims: Relevant claims from the knowledge base
            patterns: Detected patterns
            contradictions: Known contradictions
            gaps: Identified knowledge gaps

        Returns:
            SynthesisReport with full analysis
        """
        claims_str = json.dumps(claims[:30], indent=2)
        patterns_str = json.dumps(patterns[:15], indent=2)
        contradictions_str = json.dumps(contradictions[:10], indent=2) if contradictions else "None identified"
        gaps_str = json.dumps(gaps[:10], indent=2) if gaps else "None identified"

        prompt = f"""Generate a comprehensive scientific synthesis report on: {topic}

Available Evidence:

CLAIMS ({len(claims)} total, showing top 30):
{claims_str}

PATTERNS ({len(patterns)} total, showing top 15):
{patterns_str}

CONTRADICTIONS:
{contradictions_str}

KNOWLEDGE GAPS:
{gaps_str}

Generate a synthesis report with:
1. title: A descriptive title
2. executive_summary: 2-3 paragraph overview
3. key_findings: List of 5-10 main findings
4. cross_domain_insights: List of cross-domain connections discovered
5. contradictions_noted: List of unresolved contradictions
6. knowledge_gaps: List of important gaps in understanding
7. future_directions: List of recommended research directions
8. full_report: Full 500-1000 word report in academic prose

Return as JSON with these fields."""

        try:
            result = await self.backend.generate_json(prompt, self.SYSTEM_SYNTHESIS)

            return SynthesisReport(
                title=result.get("title", f"Synthesis Report: {topic}"),
                executive_summary=result.get("executive_summary", ""),
                key_findings=result.get("key_findings", []),
                cross_domain_insights=result.get("cross_domain_insights", []),
                contradictions_noted=result.get("contradictions_noted", []),
                knowledge_gaps=result.get("knowledge_gaps", []),
                future_directions=result.get("future_directions", []),
                full_report=result.get("full_report", "")
            )

        except Exception as e:
            logger.error(f"LLM synthesis report failed: {e}")
            return SynthesisReport(
                title=f"Synthesis Report: {topic}",
                executive_summary=f"Report generation failed: {e}",
                key_findings=[],
                cross_domain_insights=[],
                contradictions_noted=[],
                knowledge_gaps=[],
                future_directions=[],
                full_report=""
            )

    async def resolve_entity(
        self,
        entity: str,
        context: str,
        candidates: List[str] = None
    ) -> Tuple[str, float]:
        """
        Resolve an ambiguous entity mention.

        Args:
            entity: The entity mention to resolve
            context: Surrounding context
            candidates: Optional list of candidate resolutions

        Returns:
            Tuple of (resolved entity, confidence)
        """
        candidates_str = json.dumps(candidates) if candidates else "None provided"

        prompt = f"""Resolve the following entity mention to its canonical form.

Entity: {entity}
Context: {context[:500]}
Candidate resolutions: {candidates_str}

Return JSON: {{"resolved_entity": "...", "confidence": 0.0-1.0, "reasoning": "..."}}"""

        try:
            result = await self.backend.generate_json(prompt)
            return (
                result.get("resolved_entity", entity),
                float(result.get("confidence", 0.5))
            )
        except Exception as e:
            logger.error(f"Entity resolution failed: {e}")
            return (entity, 0.0)

    async def summarize_claim_cluster(
        self,
        claims: List[Dict[str, Any]]
    ) -> str:
        """
        Summarize a cluster of related claims.

        Args:
            claims: List of related claims

        Returns:
            Natural language summary
        """
        claims_str = json.dumps(claims[:20], indent=2)

        prompt = f"""Summarize the following cluster of related scientific claims in 2-3 sentences.
Focus on the main finding and any contradictions or caveats.

Claims:
{claims_str}

Provide a concise, academic summary."""

        try:
            return await self.backend.generate(prompt)
        except Exception as e:
            logger.error(f"Claim summarization failed: {e}")
            return "Summary unavailable."


# Convenience functions
def get_llm_integration(config: LLMConfig = None) -> LLMIntegration:
    """Get an LLMIntegration instance."""
    return LLMIntegration(config)


async def extract_claims_llm(
    text: str,
    title: str = "",
    domains: List[str] = None,
    config: LLMConfig = None
) -> List[ExtractedClaimLLM]:
    """Extract claims from text using LLM."""
    llm = get_llm_integration(config)
    return await llm.extract_claims(text, title, domains)


async def generate_hypotheses_llm(
    patterns: List[Dict],
    claims: List[Dict],
    num_hypotheses: int = 5,
    config: LLMConfig = None
) -> List[GeneratedHypothesis]:
    """Generate hypotheses from patterns."""
    llm = get_llm_integration(config)
    return await llm.generate_hypotheses(patterns, claims, num_hypotheses)


async def detect_analogies_llm(
    domain_a: str,
    domain_b: str,
    claims_a: List[Dict],
    claims_b: List[Dict],
    config: LLMConfig = None
) -> List[CrossDomainAnalogy]:
    """Detect cross-domain analogies."""
    llm = get_llm_integration(config)
    return await llm.detect_analogies(domain_a, domain_b, claims_a, claims_b)


async def generate_synthesis_report_llm(
    topic: str,
    claims: List[Dict],
    patterns: List[Dict],
    config: LLMConfig = None
) -> SynthesisReport:
    """Generate synthesis report."""
    llm = get_llm_integration(config)
    return await llm.generate_synthesis_report(topic, claims, patterns)
